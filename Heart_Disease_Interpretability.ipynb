{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61531eb2",
   "metadata": {},
   "source": [
    "# ü´Ä Heart Disease Prediction and Interpretability\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline applied to the UCI Heart Disease dataset. All models and tools are implemented from scratch, including:\n",
    "\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- LIME (Local Interpretable Model-Agnostic Explanations)\n",
    "\n",
    "The goal is both predictive performance and interpretability ‚Äî understanding **why** the model made a prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09baa35f",
   "metadata": {},
   "source": [
    "## üîß Setup and Imports\n",
    "\n",
    "We begin by importing all required modules from our custom `courselib` framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c4e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Heart Disease Prediction and Interpretability\n",
    "# Using custom Logistic Regression, Decision Tree, Random Forest, and LIME\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "from courselib.utils.loaders import load_heart_data\n",
    "from courselib.utils.preprocessing import preprocess_dataframe\n",
    "from courselib.utils.normalization import minmax_normalize\n",
    "from courselib.utils.splits import train_test_split\n",
    "from courselib.utils.metrics import accuracy, f1_score\n",
    "from courselib.models.logistic import LogisticRegression\n",
    "from courselib.models.tree import DecisionTree\n",
    "from courselib.models.forest import RandomForest\n",
    "from courselib.optimizers import GDOptimizer\n",
    "from courselib.explain.lime import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a5f36",
   "metadata": {},
   "source": [
    "## üì• Load and Preprocess Data\n",
    "\n",
    "We use the UCI Heart Disease dataset, which includes patient data (e.g. age, sex, cholesterol, etc.) and a target variable indicating presence (1-4) or absence (0) of heart disease (Source: https://archive.ics.uci.edu/dataset/45/heart+disease). \n",
    "\n",
    "> ### üí° Binary Transformation\n",
    ">\n",
    "> The target ranges from 0 to 4:\n",
    "> - 0 = no presence of heart disease\n",
    "> - 1‚Äì4 = presence of heart disease  \n",
    ">  \n",
    "> Our research focuses on the binary classification task: **presence (1‚Äì4) vs. absence (0)** as described in the dataset information.\n",
    "> So we convert:\n",
    ">\n",
    "> $$\n",
    "> y = \\begin{cases}\n",
    ">     0 & \\text{if } y = 0 \\\\\\\\\n",
    ">     1 & \\text{if } y \\in \\{1, 2, 3, 4\\}\n",
    "> \\end{cases}\n",
    "> $$\n",
    "\n",
    "Steps:\n",
    "- Fetch dataset\n",
    "- Convert multiclass to binary target\n",
    "- Encode categorical features\n",
    "- Normalize numerical features to [0, 1] range\n",
    "- Split into training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d71594e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.indexing._iLocIndexer at 0x18a5bb1c220>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e65517",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of '_iLocIndexer' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m X_df, y_series \u001b[38;5;241m=\u001b[39m load_heart_data()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert target to binary: 0 (no disease), 1 (disease)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y_series \u001b[38;5;241m=\u001b[39m (\u001b[43my_series\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Combine and preprocess\u001b[39;00m\n\u001b[0;32m      8\u001b[0m X, y \u001b[38;5;241m=\u001b[39m preprocess_dataframe(pd\u001b[38;5;241m.\u001b[39mconcat([X_df, y_series\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of '_iLocIndexer' and 'int'"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "X_df, y_series = load_heart_data()\n",
    "\n",
    "# Convert target to binary: 0 (no disease), 1 (disease)\n",
    "y_series = (y_series > 0).astype(int)\n",
    "\n",
    "# Combine and preprocess\n",
    "X, y = preprocess_dataframe(pd.concat([X_df, y_series.rename(\"target\")], axis=1))\n",
    "\n",
    "# Normalize features\n",
    "X = minmax_normalize(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, seed=42)\n",
    "print('Training data split as follows:')\n",
    "print(f'  Training data samples: {len(X_train)}')\n",
    "print(f'      Test data samples: {len(X_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a36981",
   "metadata": {},
   "source": [
    "## üìà Logistic Regression\n",
    "\n",
    "We implement logistic regression using gradient descent.\n",
    "\n",
    "> ### üí° Model and Loss\n",
    ">\n",
    "> The model computes probabilities as:\n",
    ">\n",
    "> $$\n",
    "> \\hat{y}_i = \\sigma(w^T x_i + b), \\quad \\text{where} \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "> $$\n",
    ">\n",
    "> The loss function is binary cross-entropy:\n",
    ">\n",
    "> $$\n",
    "> \\mathcal{L}(w, b) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "> $$\n",
    ">\n",
    "> Optimized using gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ecc64",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "return arrays must be of ArrayType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GDOptimizer(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m      2\u001b[0m logreg \u001b[38;5;241m=\u001b[39m LogisticRegression(\n\u001b[0;32m      3\u001b[0m     w\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), \n\u001b[0;32m      4\u001b[0m     b\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \n\u001b[0;32m      5\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[0;32m      6\u001b[0m     penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mlogreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m y_pred_logreg \u001b[38;5;241m=\u001b[39m logreg(X_test)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogistic Regression Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy(y_test, y_pred_logreg))\n",
      "File \u001b[1;32mc:\\Users\\Chris\\Documents\\Python-Projects\\Heart_Disease_Interpretability\\courselib\\models\\base.py:63\u001b[0m, in \u001b[0;36mTrainableModel.fit\u001b[1;34m(self, X, y, num_epochs, batch_size, compute_metrics, metrics_dict)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m     62\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad(X[idx], y[idx])\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compute_metrics:\n\u001b[0;32m     66\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(X, y, metrics_dict)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\Documents\\Python-Projects\\Heart_Disease_Interpretability\\courselib\\optimizers.py:43\u001b[0m, in \u001b[0;36mGDOptimizer.update\u001b[1;34m(self, params, grads)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: return arrays must be of ArrayType"
     ]
    }
   ],
   "source": [
    "optimizer = GDOptimizer(learning_rate=1e-4)\n",
    "logreg = LogisticRegression(\n",
    "    w=np.zeros(X_train.shape[1]), \n",
    "    b=0.0, \n",
    "    optimizer=optimizer, \n",
    "    penalty=\"none\"\n",
    ")\n",
    "logreg.fit(X_train, y_train, num_epochs=100)\n",
    "\n",
    "y_pred_logreg = logreg(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy(y_test, y_pred_logreg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d67a438c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "return arrays must be of ArrayType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GDOptimizer(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(w, b, optimizer)\n\u001b[1;32m----> 8\u001b[0m metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\Documents\\Python-Projects\\Heart_Disease_Interpretability\\courselib\\models\\base.py:63\u001b[0m, in \u001b[0;36mTrainableModel.fit\u001b[1;34m(self, X, y, num_epochs, batch_size, compute_metrics, metrics_dict)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m     62\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad(X[idx], y[idx])\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compute_metrics:\n\u001b[0;32m     66\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(X, y, metrics_dict)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\Documents\\Python-Projects\\Heart_Disease_Interpretability\\courselib\\optimizers.py:43\u001b[0m, in \u001b[0;36mGDOptimizer.update\u001b[1;34m(self, params, grads)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: return arrays must be of ArrayType"
     ]
    }
   ],
   "source": [
    "from courselib.optimizers import GDOptimizer\n",
    "\n",
    "w = np.zeros(X_train.shape[1])\n",
    "b = 0\n",
    "optimizer = GDOptimizer(learning_rate=1e-2)\n",
    "\n",
    "model = LogisticRegression(w, b, optimizer)\n",
    "metrics_history = model.fit(X_train,y_train,num_epochs=500, batch_size=len(X_train), compute_metrics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf680f",
   "metadata": {},
   "source": [
    "## üå≥ Decision Tree\n",
    "\n",
    "Decision trees recursively split the data to reduce impurity and create interpretable decision rules.\n",
    "\n",
    "> ### üí° Impurity Measures\n",
    ">\n",
    "> Gini impurity:\n",
    "> $$\n",
    "> G = 1 - \\sum_{k=1}^K p_k^2\n",
    "> $$\n",
    ">\n",
    "> Entropy:\n",
    "> $$\n",
    "> H = -\\sum_{k=1}^K p_k \\log(p_k)\n",
    "> $$\n",
    ">\n",
    "> A split is chosen to minimize weighted impurity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85f42662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7377049180327869\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(max_depth=4)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy(y_test, y_pred_tree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b687f6",
   "metadata": {},
   "source": [
    "## üå≤ Random Forest\n",
    "\n",
    "Random forests are ensembles of decision trees, trained on random subsets of the data and features.\n",
    "\n",
    "> ### üí° Key Idea\n",
    ">\n",
    "> Combine multiple weak learners (trees) to create a strong learner.\n",
    "> Each tree votes, and the majority decision is the output.\n",
    ">\n",
    "> This improves generalization and reduces variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b776610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7704918032786885\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForest(n_estimators=5, max_depth=4)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy(y_test, y_pred_forest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6697c",
   "metadata": {},
   "source": [
    "## üîç LIME: Model Interpretability\n",
    "\n",
    "LIME explains individual predictions by approximating the model locally with a simpler interpretable model.\n",
    "\n",
    "> ### üí° How LIME Works\n",
    ">\n",
    "> 1. Sample points around the instance using noise\n",
    "> 2. Get predictions from the black-box model\n",
    "> 3. Fit a weighted linear model (e.g. ridge regression)\n",
    "> 4. Interpret feature weights of this surrogate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05cd2ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m instance \u001b[38;5;241m=\u001b[39m X_test[\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      3\u001b[0m predict_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray([logreg\u001b[38;5;241m.\u001b[39mdecision_function(x)])\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m----> 4\u001b[0m weights, idx \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop LIME Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat, w \u001b[38;5;129;01min\u001b[39;00m explainer\u001b[38;5;241m.\u001b[39mas_list(weights, idx, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Chris\\Documents\\Python-Projects\\Heart_Disease_Interpretability\\courselib\\explain\\lime.py:39\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[1;34m(self, instance, model_predict_fn, num_samples)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Step 2: Get model predictions\u001b[39;00m\n\u001b[0;32m     38\u001b[0m preds \u001b[38;5;241m=\u001b[39m model_predict_fn(perturbed)\n\u001b[1;32m---> 39\u001b[0m preds \u001b[38;5;241m=\u001b[39m preds \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# use class 1 probability if applicable\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Step 3: Compute distances and weights\u001b[39;00m\n\u001b[0;32m     42\u001b[0m distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm((perturbed \u001b[38;5;241m-\u001b[39m instance) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_std, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "explainer = LimeTabularExplainer(X_train, feature_names=X_df.columns.tolist())\n",
    "instance = X_test[5]\n",
    "predict_fn = lambda x: np.array([logreg.decision_function(x)]).T\n",
    "weights, idx = explainer.explain_instance(instance, predict_fn, num_samples=300)\n",
    "\n",
    "print(\"Top LIME Features:\")\n",
    "for feat, w in explainer.as_list(weights, idx, top_k=5):\n",
    "    print(f\"{feat}: {w:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386770d",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "We implemented and interpreted multiple models from scratch:\n",
    "\n",
    "- üßÆ Logistic Regression (with gradients and sigmoid)\n",
    "- üå≥ Decision Tree (using impurity criteria)\n",
    "- üå≤ Random Forest (ensemble of trees)\n",
    "- üîç LIME (local explanations with RidgeRegression)\n",
    "\n",
    "All within a modular framework designed for transparency and learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d9ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
